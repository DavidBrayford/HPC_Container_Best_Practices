# Build Systems and Configurations

One of the main issues faced by both the data center and end user is how to configure and build HPC container images for specific cluster and hardware setups. Typically, the two methods most widely used are for the end user to build the container image and test it on their own system and copy the resultant HPC container image to the cluster.

Unfortunately, this method has some significant drawbacks, as an increasing number of HPC systems have different types of CPU’s with different instruction set architecture (ISA) than what the user have on their desktops or laptops. As well as the differences in hardware between the cluster and the end users own system, the software tools such as compilers and libraries used to generate the executables within the container are likely to be different, which results in a suboptimal performing libraries and/or executables that make up the containerized workflow being generated.

In addition, there are significant security issues associated with allowing unknown containers to run on HPC clusters. Firstly, unauthorized code could be executing on the HPC cluster in a disguised manner within the intended workflow. An example of this type of exploited was when a well-used open source molecular dynamics package within the clusters module system was replaced with a modified version that executed cryptocurrency mining during the applications startup phase.

The second commonly used method for configuring and building HPC containers is to build the container on the system, which solves the problems associated with different hardware configurations and software tools between the HPC cluster and the end-users own system. Unfortunately, this method introduces other significant issues including how to build the container in a non-privileged manner as a “normal” user on the cluster. Also, for clusters that don’t have a direct connection to the internet downloading software and source code from external servers becomes very difficult.

One potential method to overcome the issues described above for building HPC containers would be for the data center to provide a build system consisting of one or more nodes not connected to the HPC cluster with identical hardware configurations to the HPC cluster. These builder nodes would run a virtualized software environment such as a virtual machine. This would enable the user to spin up a virtual machine, build their container from a Dockerfile and copy the image to a staging location which can be accessed by the HPC cluster. However, this does not solve the issue of endusers deliberately or accidentally incorporating modified code or binaries into their container image.

For secure sites additional constraints are applied to container image building and deploying. One such constraint would be the geographics location of the remote server where the source code and/or binaries are stored. Connecting and downloading data from a server based in sanctioned foreign country would be a major security breach and would result in a criminal investigation and severe sanctions for the users even if it was accidental and weren’t aware where the server was located, because the final server address was not known. To avoid this scenario often trusted servers are employed to store data that would normally be found on public facing servers.

Another such constraint could be a limitation on binaries being used in the building of the container image as well as only allowing authorized people to build the container images. Meaning that the data center must provide a container build service, which verifies the container recipe (Dockerfile) provided by the user and builds the container image for the user. If these images created by the data center are deployed by multiple users, they can be stored in a registry for the specific cluster for reuse.
