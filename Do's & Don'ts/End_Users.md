# Do’s
Always build HPC containers from recipes (Dockerfiles). As it allows for the verification of the container image by inspecting the text file and ensures that no nefarious code can be easily incorporated into the container image. It also allows for the container workflow recipe to be modified for different clusters that have different hardware, software or security policies. Making it easier to generate performant containerized workflows because the image can be built for the specific hardware and software configuration of the cluster you want to run on. Another advantage of using Dockerfile recipes is that the text file takes up far less storage than the container image binary, which is ideal for transferring to a new system, it only takes a few seconds to copy a small text file to a different cluster or data center.

If the data center container service provides verified templates and recipes, the end user should use those recipes as a base, which they can incorporate their workflows. Or for common HPC applications and workflows the data center might already have verified recipes or even prebuilt HPC container images for the clusters. These base template recipes are often easier and quicker to debug as all the system configurations have been provided by the data center and the users only has to verify that their workflow is correct and compliant with the policies of the cluster and data center. We decides to go with Dockerfiles over sif files because all HPC container technologies that I am aware of can use Dockerfiles in their container build process. While not all HPC container technologies will build an image from a sif file by default.

Finally, the end user needs to understand how the HPC container runtime works. Otherwise it is highly likely that either the container image is not correctly configured and the containerized workflow doesn’t execute correctly on the cluster. An example of this includes installing the applications or data in a directory in the container image that gets mounted from the host into the container image at runtime. Another issue that can arise is the end-user forgets that the container image uses the underlying host environment and performs an operation inside the container runtime that causes problems on the host.

# Don’ts: 
Don’t bring a container image from another HPC cluster, system or repository because HPC containers are not portable between clusters if they don’t have the same hardware & software configurations, operational modes and security policies. The container images from other cluster can be difficult to get working as the CPU can have different ISA’s and GPU’s from different vendors, which have differing software stacks and programming environments. In addition, to that the systems might have different types of MPI, which are not ABI compatible and differing high speed interconnect software. As well as different operational modes and security policies. Finally, even if the user can get the container to execute correctly on the cluster, it is highly likely that the workflow won’t be performant.

Avoid storing data within the container and try to keep the container and data separate if possible. Firstly, the container build system might not be able to store all the data that is needed containerized workflow. For example, If the user is building the containers on a laptop, desktop or a standalone node storing terabytes of data required by the workflow might not be possible. In addition, to storing the data on the build system, the user needs to transfer their container image to the data center and HPC cluster. Transferring terabytes of data from the users build system to the data center will be very slow and potentially not feasible over a standard internet connection. It is better to separate the application and data, so that the applications are located within the container image and the data is stored on the filesystem attached to the cluster and mounted within the container image. This allow to data to be transferred to the datacenter separately from the container image and the data can be stored in a location that is accessible by more than one cluster in the data center. Also, the data can be stored in higher performant partitions of the network file system within the data center.

Don’t install everything you might ever need in the container image, as we want minimal size container images. Not only are they easier to transfer across networks, but takes up less space on the container build system and also on the filesystem of the data center or cluster. The build time for small container images will often take less time. Also, on some clusters the compute nodes are provisioned to be stateless, which means that they don’t use hard disks and everything is stored and run from the nodes local RAM. So if the users job runtime memory requirements exceed the amount of RAM available on the node for the job the node will crash. Finally, it is recommended to delete all source code and temporary files that are used to install and build the workflow, as this might reduce the possibility that containerized workflow doesn’t crash by exceeding the RAM available on the node.

Don’t require additional components to be installed by the data center on the system. As changes to a production system in operation is often not possible and even if it is possible the data center won’t make the requested modifications or they will take a long time to implement the requested changes, because they would need to assign resources such as staff to analyze the impact of the requested modification on the system and then have to implement, test and verify the changes. This might take several months or even years, so it is likely that the request will be rejected.

Don’t perform an operation inside a container that would crash the system if executed outside of a container. Remember that the containerized image interfaces with the host systems system software to access the hardware and system software. So if an operation crashes the system when running directly on the host, expect the same to happen if the same operation is executed from inside a container image. For example, a job running directly on the cluster that opens millions of small files at once will crash the network file system, running the same job from with a container image on the same cluster will also crash the network file system.

Don’t expect the containerized workflows to behave in the same manner if the underlying host system has changed. This is because the containers are dependent on the underlying host system. So when something changes to the underlying host system either by changing the hardware or software the execution of the containerized workflow might be different and you are no longer able to reproduce the same results or the same performance. An example of this was after the network file system received a new software patch the users of a containerized workflow suddenly notice that their workflow could scale to a larger number of nodes.

If the user wants to incorporate licenced software in their containerized workflow: Don’t install licenced software inside a container image which can be executed outside of the software licencing agreement. This can cause legal issues for anyone who has entered into the licencing agreement and can result in the licencing agreement being terminated, fines and other legal problems. A better approach is for the end-user to use the licenced software that is installed on the host system and this can be achieved by mounting the relevant directories where the software is installed on the host system into the container.

If the container runtime mounts host directories inside the container by default. Don’t install anything in those mounted directory inside the container during the building of the image. Or if possible override the default mounting behavior of the container runtime. Some HPC container technologies such as Charliecloud mount the hosts $HOME and other host directories inside the container at runtime. So it is important for the user to avoid installing software and data inside that directory.
